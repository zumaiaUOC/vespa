{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nkDv5dppU6B"
   },
   "source": [
    "# HIVE algorithm **Kopuru Vespa Velutina Competition**\n",
    "\n",
    "Purpose: to process the weather data from Biscay's weather stations into a workable dataset.\n",
    "\n",
    "Output: METEO dataset *(WBds02_METEO.csv)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:23.359051Z",
     "start_time": "2021-06-06T20:18:22.963625Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:26.603988Z",
     "start_time": "2021-06-06T20:18:26.145832Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         0         1         2     3                                      4  \\\n",
       "0    COD.  ESTACION  cota (m)   ENE                                    FEB   \n",
       "1     NaN       NaN       NaN   NaN  PRECIPITACIÓN MÁXIMA EN UN DÍA (l/m2)   \n",
       "2    KOD.  ESTAZIOA  kota (m)   URT                                    OTS   \n",
       "3    C076  Abetxuko       510  19.1                                   55.2   \n",
       "4    C056   Alegria       545    25                                   52.3   \n",
       "..    ...       ...       ...   ...                                    ...   \n",
       "104  C064   Zarautz        80     0                                      2   \n",
       "105  C028    Zegama       520     0                                     12   \n",
       "106  C029  Zizurkil       149     0                                      6   \n",
       "107  COD.  ESTACION  cota (m)   ENE                                    FEB   \n",
       "108   NaN       NaN       NaN   NaN                                    NaN   \n",
       "\n",
       "        5               6     7     8    9    10    11   12    13   14    15  \\\n",
       "0     MAR             ABR   MAY   JUN  JUL   AGO   SET  OCT   NOV  DIC   MAX   \n",
       "1     NaN             NaN   NaN   NaN  NaN   NaN   NaN  NaN   NaN  NaN   NaN   \n",
       "2     MAR             API   MAI   EKA  UZT   ABU   IRA  URR   AZA  ABE   MAX   \n",
       "3    24.7             7.3  10.5  18.1  6.1     4  11.8  5.9  42.5  4.8  55.2   \n",
       "4    22.8             9.1   5.7  24.6  3.1  10.5     8  5.1  38.1    3  52.3   \n",
       "..    ...             ...   ...   ...  ...   ...   ...  ...   ...  ...   ...   \n",
       "104     0               0     0     0    0     0     0    0     0    0     2   \n",
       "105     2               0     0     0    0     0     0    0     0    1    15   \n",
       "106     0               0     0     0    0     0     0    0     0    0     6   \n",
       "107   MAR             ABR   MAY   JUN  JUL   AGO   SET  OCT   NOV  DIC  SUMA   \n",
       "108   NaN  DÍAS DE HELADA   NaN   NaN  NaN   NaN   NaN  NaN   NaN  NaN   NaN   \n",
       "\n",
       "                                                   new  \n",
       "0    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...  \n",
       "1    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...  \n",
       "2    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...  \n",
       "3    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...  \n",
       "4    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...  \n",
       "..                                                 ...  \n",
       "104                      ds06-2018_DÍAS DE HELADA 2018  \n",
       "105                      ds06-2018_DÍAS DE HELADA 2018  \n",
       "106                      ds06-2018_DÍAS DE HELADA 2018  \n",
       "107                      ds06-2018_DÍAS DE HELADA 2018  \n",
       "108                      ds06-2018_DÍAS DE HELADA 2018  \n",
       "\n",
       "[17877 rows x 17 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'../Input_open_data' # use your path\n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "df = pd.concat([pd.read_csv(fp, header=None, sep=';').assign(new=os.path.basename(fp).split('.')[0]) for fp in files])\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:29.381565Z",
     "start_time": "2021-06-06T20:18:29.343282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of      COD.  ESTACION  cota (m)   ENE                                    FEB  \\\n",
       "0    COD.  ESTACION  cota (m)   ENE                                    FEB   \n",
       "1     NaN       NaN       NaN   NaN  PRECIPITACIÓN MÁXIMA EN UN DÍA (l/m2)   \n",
       "2    KOD.  ESTAZIOA  kota (m)   URT                                    OTS   \n",
       "3    C076  Abetxuko       510  19.1                                   55.2   \n",
       "4    C056   Alegria       545    25                                   52.3   \n",
       "..    ...       ...       ...   ...                                    ...   \n",
       "104  C064   Zarautz        80     0                                      2   \n",
       "105  C028    Zegama       520     0                                     12   \n",
       "106  C029  Zizurkil       149     0                                      6   \n",
       "107  COD.  ESTACION  cota (m)   ENE                                    FEB   \n",
       "108   NaN       NaN       NaN   NaN                                    NaN   \n",
       "\n",
       "      MAR             ABR   MAY   JUN  JUL   AGO   SET  OCT   NOV  DIC   MAX  \\\n",
       "0     MAR             ABR   MAY   JUN  JUL   AGO   SET  OCT   NOV  DIC   MAX   \n",
       "1     NaN             NaN   NaN   NaN  NaN   NaN   NaN  NaN   NaN  NaN   NaN   \n",
       "2     MAR             API   MAI   EKA  UZT   ABU   IRA  URR   AZA  ABE   MAX   \n",
       "3    24.7             7.3  10.5  18.1  6.1     4  11.8  5.9  42.5  4.8  55.2   \n",
       "4    22.8             9.1   5.7  24.6  3.1  10.5     8  5.1  38.1    3  52.3   \n",
       "..    ...             ...   ...   ...  ...   ...   ...  ...   ...  ...   ...   \n",
       "104     0               0     0     0    0     0     0    0     0    0     2   \n",
       "105     2               0     0     0    0     0     0    0     0    1    15   \n",
       "106     0               0     0     0    0     0     0    0     0    0     6   \n",
       "107   MAR             ABR   MAY   JUN  JUL   AGO   SET  OCT   NOV  DIC  SUMA   \n",
       "108   NaN  DÍAS DE HELADA   NaN   NaN  NaN   NaN   NaN  NaN   NaN  NaN   NaN   \n",
       "\n",
       "    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2) 2016  \n",
       "0    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...   \n",
       "1    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...   \n",
       "2    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...   \n",
       "3    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...   \n",
       "4    ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2)...   \n",
       "..                                                 ...   \n",
       "104                      ds06-2018_DÍAS DE HELADA 2018   \n",
       "105                      ds06-2018_DÍAS DE HELADA 2018   \n",
       "106                      ds06-2018_DÍAS DE HELADA 2018   \n",
       "107                      ds06-2018_DÍAS DE HELADA 2018   \n",
       "108                      ds06-2018_DÍAS DE HELADA 2018   \n",
       "\n",
       "[17877 rows x 17 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns=df.iloc[0], inplace=True)\n",
    "df.columns\n",
    "df.rename(columns={'DÍAS DE HELADA 2016': 'new'}, inplace= True)\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:30.883515Z",
     "start_time": "2021-06-06T20:18:30.822777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17508 entries, 3 to 106\n",
      "Data columns (total 18 columns):\n",
      "COD.                                                   17508 non-null object\n",
      "ESTACION                                               10717 non-null object\n",
      "cota (m)                                               10605 non-null object\n",
      "ENE                                                    6080 non-null object\n",
      "FEB                                                    5987 non-null object\n",
      "MAR                                                    6016 non-null object\n",
      "ABR                                                    5922 non-null object\n",
      "MAY                                                    5936 non-null object\n",
      "JUN                                                    5934 non-null object\n",
      "JUL                                                    5934 non-null object\n",
      "AGO                                                    5934 non-null object\n",
      "SET                                                    5925 non-null object\n",
      "OCT                                                    5928 non-null object\n",
      "NOV                                                    5925 non-null object\n",
      "DIC                                                    5916 non-null object\n",
      "MAX                                                    5750 non-null object\n",
      "ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2) 2016    17508 non-null object\n",
      "new                                                    17508 non-null object\n",
      "dtypes: object(18)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# df=df.loc[~df['COD.'].isin(['KOD.','COD.' ]),:].dropna(subset=['COD.']).drop(columns=[\"cota (m)\", \"SUMA\"])\n",
    "df=df.loc[~df['COD.'].isin(['KOD.','COD.' ]),:].dropna(subset=['COD.'])\n",
    "df['new'] = df['ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2) 2016']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:31.965519Z",
     "start_time": "2021-06-06T20:18:31.910543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract year from the string  \n",
    "df['year'] = df['ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2) 2016'].str.extract('(\\d\\d\\d\\d)', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:33.117976Z",
     "start_time": "2021-06-06T20:18:33.112463Z"
    }
   },
   "outputs": [],
   "source": [
    "#Función para crear codigo_merge\n",
    "def str_join(df, sep, *cols):\n",
    "    from functools import reduce\n",
    "    return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n",
    "                 [df[col] for col in cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:34.231852Z",
     "start_time": "2021-06-06T20:18:34.202878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17508 entries, 3 to 106\n",
      "Data columns (total 19 columns):\n",
      "COD.                                                   17508 non-null object\n",
      "ESTACION                                               10717 non-null object\n",
      "cota (m)                                               10605 non-null object\n",
      "ENE                                                    6080 non-null object\n",
      "FEB                                                    5987 non-null object\n",
      "MAR                                                    6016 non-null object\n",
      "ABR                                                    5922 non-null object\n",
      "MAY                                                    5936 non-null object\n",
      "JUN                                                    5934 non-null object\n",
      "JUL                                                    5934 non-null object\n",
      "AGO                                                    5934 non-null object\n",
      "SET                                                    5925 non-null object\n",
      "OCT                                                    5928 non-null object\n",
      "NOV                                                    5925 non-null object\n",
      "DIC                                                    5916 non-null object\n",
      "MAX                                                    5750 non-null object\n",
      "ds17-2016_PRECIPITACIÓN MÁXIMA EN UN DÍA (lm2) 2016    17508 non-null object\n",
      "new                                                    17508 non-null object\n",
      "year                                                   5992 non-null object\n",
      "dtypes: object(19)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Get the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:36.978107Z",
     "start_time": "2021-06-06T20:18:36.528297Z"
    }
   },
   "outputs": [],
   "source": [
    "#Variables------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Freeze----------------------------\n",
    "freez= df[df['new'].str.contains(\"HELADA\")].drop(columns=['new'])\n",
    "freez=pd.melt(freez, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='freez')\n",
    "\n",
    "freez['merge_cod'] = str_join(freez,'_' , 'COD.','ESTACION','year', 'month')\n",
    "freez.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "cols= ['merge_cod', 'freez']\n",
    "freez= freez.reindex(columns= cols)\n",
    "\n",
    "## Rain ------------------------------\n",
    "rain= df[df['new'].str.contains(\"DÍAS DE PRECIPITACIÓN 20\")].drop(columns=['new'])\n",
    "rain=pd.melt(rain, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain')\n",
    "\n",
    "rain['merge_cod'] = str_join(rain,'_' , 'COD.','ESTACION','year', 'month')\n",
    "\n",
    "rain.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## rain_1mm----------------------------\n",
    "\n",
    "rain_1mm= df[df['new'].str.contains(\"DÍAS DE PRECIPITACIÓN IGUAL O SUPERIOR\")].drop(columns=['new'])\n",
    "rain_1mm=pd.melt(rain_1mm, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_1mm')\n",
    "\n",
    "rain_1mm['merge_cod'] = str_join(rain_1mm,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_1mm.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## rain_cum------------------------------\n",
    "rain_cum= df[df['new'].str.contains(\"PRECIPITACIÓN ACUMULADA\")].drop(columns=['new'])\n",
    "rain_cum=pd.melt(rain_cum, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_cum')\n",
    "\n",
    "rain_cum['merge_cod'] = str_join(rain_cum,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_cum.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## rain_max_10------------------------------\n",
    "rain_max_10= df[df['new'].str.contains(\"PRECIPITACIÓN MÁXIMA EN 10 MINUTOS \")].drop(columns=['new'])\n",
    "rain_max_10=pd.melt(rain_max_10, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_max_10')\n",
    "\n",
    "rain_max_10['merge_cod'] = str_join(rain_max_10,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_max_10.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## rain_max_day------------------------------\n",
    "rain_max_day= df[df['new'].str.contains(\"PRECIPITACIÓN MÁXIMA EN UN DÍA \")].drop(columns=['new'])\n",
    "rain_max_day=pd.melt(rain_max_day, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_max_day')\n",
    "\n",
    "rain_max_day['merge_cod'] = str_join(rain_max_day,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_max_day.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## hum------------------------------\n",
    "hum= df[df['new'].str.contains(\"HUMEDAD MEDIA\")].drop(columns=['new'])\n",
    "hum=pd.melt(hum, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='hum')\n",
    "\n",
    "hum['merge_cod'] = str_join(hum,'_' , 'COD.','ESTACION','year', 'month')\n",
    "hum.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## sun------------------------------\n",
    "sun= df[df['new'].str.contains(\"IRRADIACIÓN MEDIA\")].drop(columns=['new'])\n",
    "sun=pd.melt(sun, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='sun')\n",
    "\n",
    "sun['merge_cod'] = str_join(sun,'_' , 'COD.','ESTACION','year', 'month')\n",
    "sun.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## lev_max------------------------------\n",
    "lev_max= df[df['new'].str.contains(\"NIVEL MÁXIMO\")].drop(columns=['new'])\n",
    "lev_max=pd.melt(lev_max, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='lev_max')\n",
    "\n",
    "lev_max['merge_cod'] = str_join(lev_max,'_' , 'COD.','ESTACION','year', 'month')\n",
    "lev_max.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "## lev_mid------------------------------\n",
    "lev_mid= df[df['new'].str.contains(\"NIVEL MEDIO\")].drop(columns=['new'])\n",
    "lev_mid=pd.melt(lev_mid, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='lev_mid')\n",
    "\n",
    "lev_mid['merge_cod'] = str_join(lev_mid,'_' , 'COD.','ESTACION','year', 'month')\n",
    "lev_mid.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## lev_min------------------------------only 2019\n",
    "lev_min= df[df['new'].str.contains(\"NIVEL MÍNIMO\")].drop(columns=['new'])\n",
    "lev_min=pd.melt(lev_min, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='lev_min')\n",
    "\n",
    "lev_min['merge_cod'] = str_join(lev_min,'_' , 'COD.','ESTACION','year', 'month')\n",
    "lev_min.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## temp_max_abs------------------------------\n",
    "temp_max_abs= df[df['new'].str.contains(\"TEMPERATURA MÁXIMA ABSOLUTA\")].drop(columns=['new'])\n",
    "temp_max_abs=pd.melt(temp_max_abs, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_max_abs')\n",
    "\n",
    "temp_max_abs['merge_cod'] = str_join(temp_max_abs,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_max_abs.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## temp_max_avg-----------------------------\n",
    "temp_max_avg= df[df['new'].str.contains(\"TEMPERATURA MÁXIMA MEDIA\")].drop(columns=['new'])\n",
    "temp_max_avg=pd.melt(temp_max_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_max_avg')\n",
    "\n",
    "temp_max_avg['merge_cod'] = str_join(temp_max_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_max_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## temp_avg----------------------------\n",
    "temp_avg= df[df['new'].str.contains(\"TEMPERATURA MEDIA\")].drop(columns=['new'])\n",
    "temp_avg=pd.melt(temp_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_avg')\n",
    "\n",
    "temp_avg['merge_cod'] = str_join(temp_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## temp_min_abs----------------------------\n",
    "temp_min_abs= df[df['new'].str.contains(\"TEMPERATURA MÍNIMA MEDIA \")].drop(columns=['new'])\n",
    "temp_min_abs=pd.melt(temp_min_abs, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_min_abs')\n",
    "\n",
    "temp_min_abs['merge_cod'] = str_join(temp_min_abs,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_min_abs.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## wind_max----------------------------\n",
    "wind_max= df[df['new'].str.contains(\"VELOCIDAD DE LA RACHA MÁXIMA \")].drop(columns=['new'])\n",
    "wind_max=pd.melt(wind_max, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='wind_max')\n",
    "\n",
    "wind_max['merge_cod'] = str_join(wind_max,'_' , 'COD.','ESTACION','year', 'month')\n",
    "wind_max.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## wind_avg----------------------------\n",
    "wind_avg= df[df['new'].str.contains(\"VELOCIDAD MEDIA \")].drop(columns=['new'])\n",
    "wind_avg=pd.melt(wind_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='wind_avg')\n",
    "\n",
    "wind_avg['merge_cod'] = str_join(wind_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "wind_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## wind_max_avg----------------------------\n",
    "wind_max_avg= df[df['new'].str.contains(\"MEDIA DE LAS VELOCIDADES MÁXIMAS \")].drop(columns=['new'])\n",
    "wind_max_avg=pd.melt(wind_max_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='wind_max_avg')\n",
    "\n",
    "wind_max_avg['merge_cod'] = str_join(wind_max_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "wind_max_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:40.035456Z",
     "start_time": "2021-06-06T20:18:39.870043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merge_cod</th>\n",
       "      <th>freez</th>\n",
       "      <th>hum</th>\n",
       "      <th>lev_max</th>\n",
       "      <th>lev_mid</th>\n",
       "      <th>lev_min</th>\n",
       "      <th>rain</th>\n",
       "      <th>rain_1mm</th>\n",
       "      <th>rain_cum</th>\n",
       "      <th>rain_max_10</th>\n",
       "      <th>...</th>\n",
       "      <th>temp_max_avg</th>\n",
       "      <th>temp_min_abs</th>\n",
       "      <th>wind_avg</th>\n",
       "      <th>wind_max</th>\n",
       "      <th>wind_max_avg</th>\n",
       "      <th>code_merge</th>\n",
       "      <th>codigo</th>\n",
       "      <th>estacion</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C076_Abetxuko_2016_ENE</td>\n",
       "      <td>9</td>\n",
       "      <td>86.1</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>75.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C076_Abetxuko_2016_ENE</td>\n",
       "      <td>C076</td>\n",
       "      <td>Abetxuko</td>\n",
       "      <td>2016</td>\n",
       "      <td>ENE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C056_Alegria_2016_ENE</td>\n",
       "      <td>8</td>\n",
       "      <td>90.9</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>83.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>11.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>94.7</td>\n",
       "      <td>43.7</td>\n",
       "      <td>C056_Alegria_2016_ENE</td>\n",
       "      <td>C056</td>\n",
       "      <td>Alegria</td>\n",
       "      <td>2016</td>\n",
       "      <td>ENE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C035_Altube_2016_ENE</td>\n",
       "      <td>5</td>\n",
       "      <td>91.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>115.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>79.2</td>\n",
       "      <td>35.8</td>\n",
       "      <td>C035_Altube_2016_ENE</td>\n",
       "      <td>C035</td>\n",
       "      <td>Altube</td>\n",
       "      <td>2016</td>\n",
       "      <td>ENE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C001_Arkaute_2016_ENE</td>\n",
       "      <td>9</td>\n",
       "      <td>85.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>84.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>97.2</td>\n",
       "      <td>40.8</td>\n",
       "      <td>C001_Arkaute_2016_ENE</td>\n",
       "      <td>C001</td>\n",
       "      <td>Arkaute</td>\n",
       "      <td>2016</td>\n",
       "      <td>ENE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C025_Beluntza_2016_ENE</td>\n",
       "      <td>3</td>\n",
       "      <td>85.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>57.1</td>\n",
       "      <td>C025_Beluntza_2016_ENE</td>\n",
       "      <td>C025</td>\n",
       "      <td>Beluntza</td>\n",
       "      <td>2016</td>\n",
       "      <td>ENE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                merge_cod freez   hum lev_max lev_mid lev_min rain rain_1mm  \\\n",
       "0  C076_Abetxuko_2016_ENE     9  86.1   0.166   0.082     NaN   20       11   \n",
       "1   C056_Alegria_2016_ENE     8  90.9   0.652   0.287     NaN   22       12   \n",
       "2    C035_Altube_2016_ENE     5  91.9     NaN     NaN     NaN   22       15   \n",
       "3   C001_Arkaute_2016_ENE     9  85.6     NaN     NaN     NaN   19       13   \n",
       "4  C025_Beluntza_2016_ENE     3  85.2     NaN     NaN     NaN  NaN      NaN   \n",
       "\n",
       "  rain_cum rain_max_10  ... temp_max_avg temp_min_abs wind_avg wind_max  \\\n",
       "0     75.7         1.4  ...         11.4          3.0      NaN      NaN   \n",
       "1     83.6         1.1  ...         11.3          3.2     12.3     94.7   \n",
       "2    115.9         1.5  ...         12.0          4.0      7.2     79.2   \n",
       "3     84.7         1.3  ...         11.7          3.0     11.3     97.2   \n",
       "4      NaN         NaN  ...         10.1          3.7     20.5    108.0   \n",
       "\n",
       "  wind_max_avg              code_merge codigo  estacion  year month  \n",
       "0          NaN  C076_Abetxuko_2016_ENE   C076  Abetxuko  2016   ENE  \n",
       "1         43.7   C056_Alegria_2016_ENE   C056   Alegria  2016   ENE  \n",
       "2         35.8    C035_Altube_2016_ENE   C035    Altube  2016   ENE  \n",
       "3         40.8   C001_Arkaute_2016_ENE   C001   Arkaute  2016   ENE  \n",
       "4         57.1  C025_Beluntza_2016_ENE   C025  Beluntza  2016   ENE  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data= freez.merge(hum, on='merge_cod', how= 'outer' ).merge(\n",
    "    lev_max, on='merge_cod', how= 'outer' ).merge(\n",
    "    lev_mid, on='merge_cod', how= 'outer' ).merge(\n",
    "    lev_min, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_1mm, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_cum, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_max_10, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_max_day, on='merge_cod', how= 'outer' ).merge(\n",
    "    sun, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_max_abs, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_max_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_min_abs, on='merge_cod', how= 'outer' ).merge(\n",
    "    wind_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    wind_max, on='merge_cod', how= 'outer' ).merge(\n",
    "    wind_max_avg, on='merge_cod', how= 'outer' )\n",
    "\n",
    "m_data['code_merge']= m_data['merge_cod']\n",
    "m_data[['codigo',' estacion','year', 'month']]= m_data.merge_cod.str.split(\"_\",expand=True)\n",
    "m_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:42.465674Z",
     "start_time": "2021-06-06T20:18:42.460314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oscar/Documentos/MASTER UOC/Visualización de Datos/A9: Creación de la visualización y entrega del proyecto (Práctica)/Work'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:43.477619Z",
     "start_time": "2021-06-06T20:18:43.291505Z"
    }
   },
   "outputs": [],
   "source": [
    "m_data.to_csv (r'../data/estaciones.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:44.302435Z",
     "start_time": "2021-06-06T20:18:44.272800Z"
    }
   },
   "outputs": [],
   "source": [
    "#fruta\n",
    "fruta= pd.read_csv('../Input_open_data/ds04_FRUTALES-DECLARADOS-KOPURU.csv', sep=';')\n",
    "fruta.to_csv (r'../data/fruta.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:18:45.094308Z",
     "start_time": "2021-06-06T20:18:45.084567Z"
    }
   },
   "outputs": [],
   "source": [
    "#met\n",
    "met= pd.read_csv('../Input_open_data/ds05_LOCALIZACION-ESTACIONES-METEOROLOGICAS.csv', sep=';')\n",
    "met.to_csv (r'../data/met.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:19:42.576670Z",
     "start_time": "2021-06-06T20:19:42.567625Z"
    }
   },
   "outputs": [],
   "source": [
    "#apicu\n",
    "apicu= pd.read_csv('../Input_open_data/ds03_APICULTURA_COLMENAS_KOPURU.csv', sep=';')\n",
    "apicu.to_csv (r'../data/apicu.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:19:45.501513Z",
     "start_time": "2021-06-06T20:19:45.391221Z"
    }
   },
   "outputs": [],
   "source": [
    "#nido\n",
    "nido=pd.read_csv('../Input_open_data/ds02_datos-nidos-avispa-asiatica.csv')\n",
    "nido.to_csv (r'../data/nido.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:19:50.019694Z",
     "start_time": "2021-06-06T20:19:48.431613Z"
    }
   },
   "outputs": [],
   "source": [
    "from datawig import SimpleImputer\n",
    "from datawig.utils import random_split\n",
    "from sklearn.metrics import f1_score, classification_report, precision_score, recall_score, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:19:53.005392Z",
     "start_time": "2021-06-06T20:19:52.992753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['merge_cod', 'freez', 'hum', 'lev_max', 'lev_mid', 'lev_min', 'rain',\n",
       "       'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
       "       'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg',\n",
       "       'wind_max', 'wind_max_avg', 'code_merge', 'codigo', ' estacion', 'year',\n",
       "       'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seasons= pd.read_csv('D:/Bootcamp/Data/estaciones.csv')\n",
    "seasons = m_data.copy()\n",
    "seasons.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Impute the NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:43:23.567857Z",
     "start_time": "2021-06-06T20:19:57.190924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-06 22:19:57,282 [INFO]  CategoricalEncoder for column hum                                found only 28 occurrences of value 82.2\n",
      "2021-06-06 22:19:57,284 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 84.9\n",
      "2021-06-06 22:19:57,284 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 78.9\n",
      "2021-06-06 22:19:57,285 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 83.6\n",
      "2021-06-06 22:19:57,287 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 80.9\n",
      "2021-06-06 22:19:57,288 [INFO]  CategoricalEncoder for column hum                                found only 25 occurrences of value 76.1\n",
      "2021-06-06 22:19:57,288 [INFO]  CategoricalEncoder for column hum                                found only 25 occurrences of value 83.1\n",
      "2021-06-06 22:19:57,289 [INFO]  CategoricalEncoder for column hum                                found only 24 occurrences of value 80.2\n",
      "2021-06-06 22:19:57,290 [INFO]  CategoricalEncoder for column hum                                found only 24 occurrences of value 78.0\n",
      "2021-06-06 22:19:57,291 [INFO]  CategoricalEncoder for column hum                                found only 24 occurrences of value 85.2\n",
      "2021-06-06 22:19:57,292 [INFO]  CategoricalEncoder for column hum                                found only 24 occurrences of value 82.8\n",
      "2021-06-06 22:19:57,293 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 76.5\n",
      "2021-06-06 22:19:57,294 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 83.0\n",
      "2021-06-06 22:19:57,297 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 81.6\n",
      "2021-06-06 22:19:57,299 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 84.4\n",
      "2021-06-06 22:19:57,301 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 79.8\n",
      "2021-06-06 22:19:57,302 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 82.1\n",
      "2021-06-06 22:19:57,303 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 81.4\n",
      "2021-06-06 22:19:57,304 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 86.4\n",
      "2021-06-06 22:19:57,305 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 85.6\n",
      "2021-06-06 22:19:57,306 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 82.3\n",
      "2021-06-06 22:19:57,308 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 76.0\n",
      "2021-06-06 22:19:57,309 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 79.9\n",
      "2021-06-06 22:19:57,311 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 80.6\n",
      "2021-06-06 22:19:57,313 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 82.0\n",
      "2021-06-06 22:19:57,314 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 79.5\n",
      "2021-06-06 22:19:57,318 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 82.9\n",
      "2021-06-06 22:19:57,319 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 78.6\n",
      "2021-06-06 22:19:57,321 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 85.3\n",
      "2021-06-06 22:19:57,321 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 85.4\n",
      "2021-06-06 22:19:57,322 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 83.5\n",
      "2021-06-06 22:19:57,325 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 77.2\n",
      "2021-06-06 22:19:57,326 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 83.3\n",
      "2021-06-06 22:19:57,327 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 81.7\n",
      "2021-06-06 22:19:57,329 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 80.1\n",
      "2021-06-06 22:19:57,330 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 86.1\n",
      "2021-06-06 22:19:57,332 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 86.2\n",
      "2021-06-06 22:19:57,333 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 80.8\n",
      "2021-06-06 22:19:57,335 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 78.7\n",
      "2021-06-06 22:19:57,338 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 81.8\n",
      "2021-06-06 22:19:57,339 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 84.2\n",
      "2021-06-06 22:19:57,341 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 74.9\n",
      "2021-06-06 22:19:57,343 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 79.0\n",
      "2021-06-06 22:19:57,344 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 85.7\n",
      "2021-06-06 22:19:57,345 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 85.0\n",
      "2021-06-06 22:19:57,347 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 81.1\n",
      "2021-06-06 22:19:57,348 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 83.4\n",
      "2021-06-06 22:19:57,350 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 81.5\n",
      "2021-06-06 22:19:57,351 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 79.3\n",
      "2021-06-06 22:19:57,355 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 78.4\n",
      "2021-06-06 22:19:57,356 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 83.8\n",
      "2021-06-06 22:19:57,357 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 80.5\n",
      "2021-06-06 22:19:57,358 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 77.3\n",
      "2021-06-06 22:19:57,360 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 80.0\n",
      "2021-06-06 22:19:57,361 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 78.2\n",
      "2021-06-06 22:19:57,364 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 86.9\n",
      "2021-06-06 22:19:57,365 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 80.7\n",
      "2021-06-06 22:19:57,367 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 75.3\n",
      "2021-06-06 22:19:57,369 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 78.8\n",
      "2021-06-06 22:19:57,370 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 82.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-06 22:19:57,371 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 80.3\n",
      "2021-06-06 22:19:57,372 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 79.6\n",
      "2021-06-06 22:19:57,373 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 76.2\n",
      "2021-06-06 22:19:57,374 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 84.1\n",
      "2021-06-06 22:19:57,376 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 86.7\n",
      "2021-06-06 22:19:57,379 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 76.9\n",
      "2021-06-06 22:19:57,380 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 87.4\n",
      "2021-06-06 22:19:57,381 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 81.9\n",
      "2021-06-06 22:19:57,382 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 76.4\n",
      "2021-06-06 22:19:57,382 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 77.5\n",
      "2021-06-06 22:19:57,384 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 86.5\n",
      "2021-06-06 22:19:57,385 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 77.0\n",
      "2021-06-06 22:19:57,386 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 82.7\n",
      "2021-06-06 22:19:57,387 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 77.1\n",
      "2021-06-06 22:19:57,388 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 75.5\n",
      "2021-06-06 22:19:57,389 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 81.3\n",
      "2021-06-06 22:19:57,391 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 79.4\n",
      "2021-06-06 22:19:57,392 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 84.6\n",
      "2021-06-06 22:19:57,395 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 83.7\n",
      "2021-06-06 22:19:57,396 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 73.8\n",
      "2021-06-06 22:19:57,398 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 74.8\n",
      "2021-06-06 22:19:57,400 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 73.9\n",
      "2021-06-06 22:19:57,401 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 77.7\n",
      "2021-06-06 22:19:57,402 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 74.1\n",
      "2021-06-06 22:19:57,405 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 77.6\n",
      "2021-06-06 22:19:57,406 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 86.6\n",
      "2021-06-06 22:19:57,408 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 73.0\n",
      "2021-06-06 22:19:57,410 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 83.9\n",
      "2021-06-06 22:19:57,411 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 73.4\n",
      "2021-06-06 22:19:57,413 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 82.5\n",
      "2021-06-06 22:19:57,414 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 77.4\n",
      "2021-06-06 22:19:57,417 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 88.5\n",
      "2021-06-06 22:19:57,418 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 81.0\n",
      "2021-06-06 22:19:57,419 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 84.5\n",
      "2021-06-06 22:19:57,425 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 74.3\n",
      "2021-06-06 22:19:57,426 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 75.4\n",
      "2021-06-06 22:19:57,429 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 75.6\n",
      "2021-06-06 22:19:57,431 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 88.2\n",
      "2021-06-06 22:19:57,433 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 81.2\n",
      "2021-06-06 22:19:57,436 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 87.1\n",
      "2021-06-06 22:19:57,543 [INFO]  NumExpr defaulting to 4 threads.\n",
      "2021-06-06 22:19:58,224 [WARNING]  Test set does not contain any ocurrences of values [84.6, 76.9, 83.6, 85.0, 80.8, 83.9, 83.7, 87.1, 80.2, 85.3] in column [hum], consider using a more representative test set.\n",
      "2021-06-06 22:19:58,257 [INFO]  \n",
      "========== start: fit model\n",
      "2021-06-06 22:19:58,260 [WARNING]  Already bound, ignoring bind()\n",
      "2021-06-06 22:20:00,812 [INFO]  Epoch[0] Batch [0-59]\tSpeed: 389.25 samples/sec\tcross-entropy=4.376117\thum-accuracy=0.007292\n",
      "2021-06-06 22:20:03,293 [INFO]  Epoch[0] Train-cross-entropy=4.378484\n",
      "2021-06-06 22:20:03,294 [INFO]  Epoch[0] Train-hum-accuracy=0.010064\n",
      "2021-06-06 22:20:03,298 [INFO]  Epoch[0] Time cost=5.032\n",
      "2021-06-06 22:20:03,338 [INFO]  Saved checkpoint to \"imputer_model/model-0000.params\"\n",
      "2021-06-06 22:20:03,542 [INFO]  Epoch[0] Validation-cross-entropy=4.354579\n",
      "2021-06-06 22:20:03,544 [INFO]  Epoch[0] Validation-hum-accuracy=0.000000\n",
      "2021-06-06 22:20:06,086 [INFO]  Epoch[1] Batch [0-59]\tSpeed: 375.73 samples/sec\tcross-entropy=4.034678\thum-accuracy=0.157292\n",
      "2021-06-06 22:20:08,554 [INFO]  Epoch[1] Train-cross-entropy=4.063725\n",
      "2021-06-06 22:20:08,555 [INFO]  Epoch[1] Train-hum-accuracy=0.111758\n",
      "2021-06-06 22:20:08,556 [INFO]  Epoch[1] Time cost=5.011\n",
      "2021-06-06 22:20:08,602 [INFO]  Saved checkpoint to \"imputer_model/model-0001.params\"\n",
      "2021-06-06 22:20:08,768 [INFO]  Epoch[1] Validation-cross-entropy=4.367689\n",
      "2021-06-06 22:20:08,769 [INFO]  Epoch[1] Validation-hum-accuracy=0.000000\n",
      "2021-06-06 22:20:11,354 [INFO]  Epoch[2] Batch [0-59]\tSpeed: 372.66 samples/sec\tcross-entropy=3.798897\thum-accuracy=0.370833\n",
      "2021-06-06 22:20:14,024 [INFO]  Epoch[2] Train-cross-entropy=3.820448\n",
      "2021-06-06 22:20:14,027 [INFO]  Epoch[2] Train-hum-accuracy=0.292373\n",
      "2021-06-06 22:20:14,028 [INFO]  Epoch[2] Time cost=5.255\n",
      "2021-06-06 22:20:14,072 [INFO]  Saved checkpoint to \"imputer_model/model-0002.params\"\n",
      "2021-06-06 22:20:14,277 [INFO]  Epoch[2] Validation-cross-entropy=4.387026\n",
      "2021-06-06 22:20:14,278 [INFO]  Epoch[2] Validation-hum-accuracy=0.017045\n",
      "2021-06-06 22:20:16,877 [INFO]  Epoch[3] Batch [0-59]\tSpeed: 370.29 samples/sec\tcross-entropy=3.570048\thum-accuracy=0.565625\n",
      "2021-06-06 22:20:19,568 [INFO]  Epoch[3] Train-cross-entropy=3.587722\n",
      "2021-06-06 22:20:19,570 [INFO]  Epoch[3] Train-hum-accuracy=0.504237\n",
      "2021-06-06 22:20:19,571 [INFO]  Epoch[3] Time cost=5.292\n",
      "2021-06-06 22:20:19,611 [INFO]  Saved checkpoint to \"imputer_model/model-0003.params\"\n",
      "2021-06-06 22:20:19,796 [INFO]  Epoch[3] Validation-cross-entropy=4.412154\n",
      "2021-06-06 22:20:19,797 [INFO]  Epoch[3] Validation-hum-accuracy=0.022727\n",
      "2021-06-06 22:20:22,515 [INFO]  Epoch[4] Batch [0-59]\tSpeed: 353.57 samples/sec\tcross-entropy=3.351927\thum-accuracy=0.713542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-06 22:20:24,977 [INFO]  Epoch[4] Train-cross-entropy=3.366716\n",
      "2021-06-06 22:20:24,979 [INFO]  Epoch[4] Train-hum-accuracy=0.659958\n",
      "2021-06-06 22:20:24,983 [INFO]  Epoch[4] Time cost=5.185\n",
      "2021-06-06 22:20:25,030 [INFO]  Saved checkpoint to \"imputer_model/model-0004.params\"\n",
      "2021-06-06 22:20:25,190 [INFO]  Epoch[4] Validation-cross-entropy=4.442218\n",
      "2021-06-06 22:20:25,191 [INFO]  Epoch[4] Validation-hum-accuracy=0.011364\n",
      "2021-06-06 22:20:27,604 [INFO]  Epoch[5] Batch [0-59]\tSpeed: 400.82 samples/sec\tcross-entropy=3.144845\thum-accuracy=0.805208\n",
      "2021-06-06 22:20:30,034 [INFO]  Epoch[5] Train-cross-entropy=3.157264\n",
      "2021-06-06 22:20:30,035 [INFO]  Epoch[5] Train-hum-accuracy=0.763242\n",
      "2021-06-06 22:20:30,037 [INFO]  Epoch[5] Time cost=4.843\n",
      "2021-06-06 22:20:30,066 [INFO]  Saved checkpoint to \"imputer_model/model-0005.params\"\n",
      "2021-06-06 22:20:30,294 [INFO]  No improvement detected for 5 epochs compared to 4.354578885165128 last error obtained: 4.476402846249667, stopping here\n",
      "2021-06-06 22:20:30,296 [INFO]  \n",
      "========== done (32.03956604003906 s) fit model\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "#Hum----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_hum = SimpleImputer(\n",
    "input_columns=['month','freez', 'temp_avg', 'rain','wind_avg','rain_1mm','rain_cum','rain_max_10','rain_max_day'],\n",
    "output_column='hum',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_hum.fit(train_df=df_train)\n",
    "predictions_hum = imputer_hum.predict(df_test)\n",
    "\n",
    "pre_hum= predictions_hum.loc[~predictions_hum['hum'].isnull(),['hum','hum_imputed'] ]\n",
    "\n",
    "#Calculate f1 score\n",
    "r2_hum = r2_score(pre_hum['hum'], pre_hum['hum_imputed'])\n",
    "msq_hum = mean_squared_error(pre_hum['hum'], pre_hum['hum_imputed'])\n",
    "\n",
    "\n",
    "#completing hum data\n",
    "\n",
    "seasons_1= imputer_hum.predict(seasons.loc[seasons['hum'].isnull(),:])\n",
    "del seasons_1[\"hum\"]\n",
    "seasons_1=seasons_1.rename(columns={'hum_imputed':'hum'}).append(seasons.loc[~seasons['hum'].isnull(),:])\n",
    "\n",
    "\n",
    "#Freez----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test= random_split(seasons_1, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_freez = SimpleImputer(\n",
    "input_columns=['month','hum', 'temp_avg', 'rain','wind_avg'],\n",
    "output_column='freez',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_freez.fit(train_df=df_train)\n",
    "predictions_freez = imputer_freez.predict(df_test)\n",
    "\n",
    "pre_freez= predictions_freez.loc[~predictions_freez['freez'].isnull(),['freez','freez_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_freez = r2_score(pre_freez['freez'], pre_freez['freez_imputed'])\n",
    "msq_freez = mean_squared_error(pre_freez['freez'], pre_freez['freez_imputed'])\n",
    "\n",
    "seasons_2= imputer_freez.predict(seasons_1.loc[seasons_1['freez'].isnull(),:])\n",
    "del seasons_2[\"freez\"]\n",
    "seasons_2=seasons_2.rename(columns={'freez_imputed':'freez'}).append(seasons_1.loc[~seasons['freez'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#Rain----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_2, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain = SimpleImputer(\n",
    "input_columns=['month','hum', 'temp_avg','wind_avg', 'freez','rain_1mm','rain_cum','rain_max_10','rain_max_day'],\n",
    "output_column='rain',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain.fit(train_df=df_train)\n",
    "predictions_rain = imputer_rain.predict(df_test)\n",
    "\n",
    "pre_rain= predictions_rain.loc[~predictions_rain['rain'].isnull(),['rain','rain_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain = r2_score(pre_rain['rain'], pre_rain['rain_imputed'])\n",
    "msq_rain = mean_squared_error(pre_rain['rain'], pre_rain['rain_imputed'])\n",
    "\n",
    "seasons_3= imputer_rain.predict(seasons_2.loc[seasons_2['rain'].isnull(),:])\n",
    "del seasons_3[\"rain\"]\n",
    "seasons_3=seasons_3.rename(columns={'rain_imputed':'rain'}).append(seasons_2.loc[~seasons['rain'].isnull(),:])\n",
    "\n",
    "\n",
    "#lev_max----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_3, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_lev_max = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','lev_mid','lev_min'],\n",
    "output_column='lev_max',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_lev_max.fit(train_df=df_train)\n",
    "predictions_lev_max = imputer_lev_max.predict(df_test)\n",
    "\n",
    "pre_lev_max= predictions_lev_max.loc[~predictions_lev_max['lev_max'].isnull(),['lev_max','lev_max_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_lev_max = r2_score(pre_lev_max['lev_max'], pre_lev_max['lev_max_imputed'])\n",
    "msq_lev_max = mean_squared_error(pre_lev_max['lev_max'], pre_lev_max['lev_max_imputed'])\n",
    "\n",
    "seasons_4= imputer_lev_max.predict(seasons_3.loc[seasons_3['lev_max'].isnull(),:])\n",
    "del seasons_4[\"lev_max\"]\n",
    "seasons_4=seasons_4.rename(columns={'lev_max_imputed':'lev_max'}).append(seasons_3.loc[~seasons['lev_max'].isnull(),:])\n",
    "\n",
    "\n",
    "#lev_mid----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_4, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_lev_mid = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','lev_min','lev_max'],\n",
    "output_column='lev_mid',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_lev_mid.fit(train_df=df_train)\n",
    "predictions_lev_mid = imputer_lev_mid.predict(df_test)\n",
    "\n",
    "pre_lev_mid= predictions_lev_mid.loc[~predictions_lev_mid['lev_mid'].isnull(),['lev_mid','lev_mid_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_lev_mid = r2_score(pre_lev_mid['lev_mid'], pre_lev_mid['lev_mid_imputed'])\n",
    "msq_lev_mid = mean_squared_error(pre_lev_mid['lev_mid'], pre_lev_mid['lev_mid_imputed'])\n",
    "\n",
    "seasons_5= imputer_lev_mid.predict(seasons_4.loc[seasons_4['lev_mid'].isnull(),:])\n",
    "del seasons_5[\"lev_mid\"]\n",
    "seasons_5=seasons_5.rename(columns={'lev_mid_imputed':'lev_mid'}).append(seasons_4.loc[~seasons['lev_mid'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#lev_min----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_5, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_lev_min = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','lev_mid','lev_max'],\n",
    "output_column='lev_min',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_lev_min.fit(train_df=df_train)\n",
    "predictions_lev_min = imputer_lev_min.predict(df_test)\n",
    "\n",
    "pre_lev_min= predictions_lev_min.loc[~predictions_lev_min['lev_min'].isnull(),['lev_min','lev_min_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_lev_min = r2_score(pre_lev_min['lev_min'], pre_lev_min['lev_min_imputed'])\n",
    "msq_lev_min = mean_squared_error(pre_lev_min['lev_min'], pre_lev_min['lev_min_imputed'])\n",
    "\n",
    "seasons_6= imputer_lev_min.predict(seasons_5.loc[seasons_5['lev_min'].isnull(),:])\n",
    "del seasons_6[\"lev_min\"]\n",
    "seasons_6=seasons_6.rename(columns={'lev_min_imputed':'lev_min'}).append(seasons_5.loc[~seasons['lev_min'].isnull(),:])\n",
    "\n",
    "\n",
    "#rain_1mm----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_6, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_1mmn = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','rain_cum','rain_max_10','rain_max_day'],\n",
    "output_column='rain_1mm',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_1mmn.fit(train_df=df_train)\n",
    "predictions_rain_1mm = imputer_rain_1mmn.predict(df_test)\n",
    "\n",
    "pre_rain_1mm= predictions_rain_1mm.loc[~predictions_rain_1mm['rain_1mm'].isnull(),['rain_1mm','rain_1mm_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_1mm = r2_score(pre_rain_1mm['rain_1mm'], pre_rain_1mm['rain_1mm_imputed'])\n",
    "msq_rain_1mm = mean_squared_error(pre_rain_1mm['rain_1mm'], pre_rain_1mm['rain_1mm_imputed'])\n",
    "\n",
    "seasons_7= imputer_rain_1mmn.predict(seasons_6.loc[seasons_6['rain_1mm'].isnull(),:])\n",
    "del seasons_7[\"rain_1mm\"]\n",
    "seasons_7=seasons_7.rename(columns={'rain_1mm_imputed':'rain_1mm'}).append(seasons_6.loc[~seasons['rain_1mm'].isnull(),:])\n",
    "\n",
    "\n",
    "#rain_cum ----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_7, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_cum  = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'freez','sun','rain_1mm','rain_max_10','rain_max_day','lev_max','lev_mid','lev_min'],\n",
    "output_column='rain_cum',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_cum.fit(train_df=df_train)\n",
    "predictions_rain_cum  = imputer_rain_cum.predict(df_test)\n",
    "\n",
    "pre_rain_cum = predictions_rain_cum.loc[~predictions_rain_cum ['rain_cum'].isnull(),['rain_cum','rain_cum_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_cum = r2_score(pre_rain_cum['rain_cum'], pre_rain_cum['rain_cum_imputed'])\n",
    "msq_rain_cum = mean_squared_error(pre_rain_cum['rain_cum'], pre_rain_cum['rain_cum_imputed'])\n",
    "\n",
    "seasons_8= imputer_rain_cum.predict(seasons_7.loc[seasons_7['rain_cum'].isnull(),:])\n",
    "del seasons_8[\"rain_cum\"]\n",
    "seasons_8=seasons_8.rename(columns={'rain_cum_imputed':'rain_cum'}).append(seasons_7.loc[~seasons['rain_cum'].isnull(),:])\n",
    "\n",
    "#rain_max_10----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_8, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_max_10 = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','rain_cum','rain_1mm','rain_max_day'],\n",
    "output_column='rain_max_10',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_max_10.fit(train_df=df_train)\n",
    "predictions_rain_max_10 = imputer_rain_max_10.predict(df_test)\n",
    "\n",
    "pre_rain_max_10= predictions_rain_max_10.loc[~predictions_rain_max_10['rain_max_10'].isnull(),['rain_max_10','rain_max_10_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_max_10 = r2_score(pre_rain_max_10['rain_max_10'], pre_rain_max_10['rain_max_10_imputed'])\n",
    "msq_rain_max_10= mean_squared_error(pre_rain_max_10['rain_max_10'], pre_rain_max_10['rain_max_10_imputed'])\n",
    "\n",
    "seasons_9= imputer_rain_max_10.predict(seasons_8.loc[seasons_8['rain_max_10'].isnull(),:])\n",
    "del seasons_9[\"rain_max_10\"]\n",
    "seasons_9=seasons_9.rename(columns={'rain_max_10_imputed':'rain_max_10'}).append(seasons_8.loc[~seasons['rain_max_10'].isnull(),:])\n",
    "\n",
    "\n",
    "#rain_max_day----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_9, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_max_day = SimpleImputer(\n",
    "input_columns=['month','hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','rain_cum','rain_1mm','rain_max_10'],\n",
    "output_column='rain_max_day',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_max_day.fit(train_df=df_train)\n",
    "predictions_rain_max_day = imputer_rain_max_day.predict(df_test)\n",
    "\n",
    "pre_rain_max_day =predictions_rain_max_day.loc[~predictions_rain_max_day['rain_max_day'].isnull(),['rain_max_day','rain_max_day_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_max_day = r2_score(pre_rain_max_day['rain_max_day'], pre_rain_max_day['rain_max_day_imputed'])\n",
    "msq_rain_max_day= mean_squared_error(pre_rain_max_day['rain_max_day'], pre_rain_max_day['rain_max_day_imputed'])\n",
    "\n",
    "seasons_10= imputer_rain_max_day.predict(seasons_9.loc[seasons_9['rain_max_day'].isnull(),:])\n",
    "del seasons_10[\"rain_max_day\"]\n",
    "seasons_10=seasons_10.rename(columns={'rain_max_day_imputed':'rain_max_day'}).append(seasons_9.loc[~seasons['rain_max_10'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#temp_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_10, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_abs','temp_max_avg','temp_min_abs'],\n",
    "output_column='temp_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_avg.fit(train_df=df_train)\n",
    "predictions_temp_avg = imputer_temp_avg.predict(df_test)\n",
    "\n",
    "pre_temp_avg =predictions_temp_avg.loc[~predictions_temp_avg['temp_avg'].isnull(),['temp_avg','temp_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_avg = r2_score(pre_temp_avg['temp_avg'], pre_temp_avg['temp_avg_imputed'])\n",
    "msq_temp_avg= mean_squared_error(pre_temp_avg['temp_avg'], pre_temp_avg['temp_avg_imputed'])\n",
    "\n",
    "seasons_11= imputer_temp_avg.predict(seasons_10.loc[seasons_10['temp_avg'].isnull(),:])\n",
    "del seasons_11[\"temp_avg\"]\n",
    "seasons_11=seasons_11.rename(columns={'temp_avg_imputed':'temp_avg'}).append(seasons_10.loc[~seasons['temp_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#temp_max_abs----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_11, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_max_abs = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_avg','temp_avg','temp_min_abs'],\n",
    "output_column='temp_max_abs',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_max_abs.fit(train_df=df_train)\n",
    "predictions_temp_max_abs = imputer_temp_max_abs.predict(df_test)\n",
    "\n",
    "pre_temp_max_abs=predictions_temp_max_abs.loc[~predictions_temp_max_abs['temp_max_abs'].isnull(),['temp_max_abs','temp_max_abs_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_max_abs= r2_score(pre_temp_max_abs['temp_max_abs'], pre_temp_max_abs['temp_max_abs_imputed'])\n",
    "msq_temp_max_abs= mean_squared_error(pre_temp_max_abs['temp_max_abs'], pre_temp_max_abs['temp_max_abs_imputed'])\n",
    "\n",
    "seasons_12= imputer_temp_max_abs.predict(seasons_11.loc[seasons_11['temp_max_abs'].isnull(),:])\n",
    "del seasons_12[\"temp_max_abs\"]\n",
    "seasons_12=seasons_12.rename(columns={'temp_max_abs_imputed':'temp_max_abs'}).append(seasons_11.loc[~seasons['temp_max_abs'].isnull(),:])\n",
    "\n",
    "\n",
    "#temp_max_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_12, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_max_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_abs','temp_avg','temp_min_abs'],\n",
    "output_column='temp_max_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_max_avg.fit(train_df=df_train)\n",
    "predictions_temp_max_avg= imputer_temp_max_avg.predict(df_test)\n",
    "\n",
    "pre_temp_max_avg=predictions_temp_max_avg.loc[~predictions_temp_max_avg['temp_max_avg'].isnull(),['temp_max_avg','temp_max_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_max_avg= r2_score(pre_temp_max_avg['temp_max_avg'], pre_temp_max_avg['temp_max_avg_imputed'])\n",
    "msq_temp_max_avg= mean_squared_error(pre_temp_max_avg['temp_max_avg'], pre_temp_max_avg['temp_max_avg_imputed'])\n",
    "\n",
    "seasons_13= imputer_temp_max_avg.predict(seasons_12.loc[seasons_12['temp_max_avg'].isnull(),:])\n",
    "del seasons_13[\"temp_max_avg\"]\n",
    "seasons_13=seasons_13.rename(columns={'temp_max_avg_imputed':'temp_max_avg'}).append(seasons_12.loc[~seasons['temp_max_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "#temp_min_abs----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_13, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_min_abs = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_abs','temp_avg','temp_max_avg'],\n",
    "output_column='temp_min_abs',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_min_abs.fit(train_df=df_train)\n",
    "predictions_temp_min_abs= imputer_temp_min_abs.predict(df_test)\n",
    "\n",
    "pre_temp_min_abs=predictions_temp_min_abs.loc[~predictions_temp_min_abs['temp_min_abs'].isnull(),['temp_min_abs','temp_min_abs_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_min_abs= r2_score(pre_temp_min_abs['temp_min_abs'], pre_temp_min_abs['temp_min_abs_imputed'])\n",
    "msq_temp_min_abs= mean_squared_error(pre_temp_min_abs['temp_min_abs'], pre_temp_min_abs['temp_min_abs_imputed'])\n",
    "\n",
    "seasons_14= imputer_temp_min_abs.predict(seasons_13.loc[seasons_13['temp_min_abs'].isnull(),:])\n",
    "del seasons_14[\"temp_min_abs\"]\n",
    "seasons_14=seasons_14.rename(columns={'temp_min_abs_imputed':'temp_min_abs'}).append(seasons_13.loc[~seasons['temp_min_abs'].isnull(),:])\n",
    "\n",
    "\n",
    "#wind_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_14, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_wind_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_max', 'rain', 'freez','sun','temp_avg', 'wind_max_avg'],\n",
    "output_column='wind_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_wind_avg.fit(train_df=df_train)\n",
    "predictions_wind_avg= imputer_wind_avg.predict(df_test)\n",
    "\n",
    "pre_wind_avg=predictions_wind_avg.loc[~predictions_wind_avg['wind_avg'].isnull(),['wind_avg','wind_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_wind_avg= r2_score(pre_wind_avg['wind_avg'], pre_wind_avg['wind_avg_imputed'])\n",
    "msq_wind_avg= mean_squared_error(pre_wind_avg['wind_avg'], pre_wind_avg['wind_avg_imputed'])\n",
    "\n",
    "seasons_15= imputer_wind_avg.predict(seasons_14.loc[seasons_14['wind_avg'].isnull(),:])\n",
    "del seasons_15[\"wind_avg\"]\n",
    "seasons_15=seasons_15.rename(columns={'wind_avg_imputed':'wind_avg'}).append(seasons_14.loc[~seasons['wind_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "#wind_max----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_15, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_wind_max = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_avg', 'wind_max_avg'],\n",
    "output_column='wind_max',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_wind_max.fit(train_df=df_train)\n",
    "predictions_wind_max= imputer_wind_max.predict(df_test)\n",
    "\n",
    "pre_wind_max=predictions_wind_max.loc[~predictions_wind_max['wind_max'].isnull(),['wind_max','wind_max_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_wind_max= r2_score(pre_wind_max['wind_max'], pre_wind_max['wind_max_imputed'])\n",
    "msq_wind_max= mean_squared_error(pre_wind_max['wind_max'], pre_wind_max['wind_max_imputed'])\n",
    "\n",
    "seasons_16= imputer_wind_max.predict(seasons_15.loc[seasons_15['wind_max'].isnull(),:])\n",
    "del seasons_16[\"wind_max\"]\n",
    "seasons_16=seasons_16.rename(columns={'wind_max_imputed':'wind_max'}).append(seasons_15.loc[~seasons['wind_max'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#wind_max_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_16, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_wind_max_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_max', 'rain', 'freez','sun','temp_avg', 'wind_max_avg'],\n",
    "output_column='wind_max_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_wind_max_avg.fit(train_df=df_train)\n",
    "predictions_wind_max_avg= imputer_wind_max_avg.predict(df_test)\n",
    "\n",
    "pre_wind_max_avg=predictions_wind_max_avg.loc[~predictions_wind_max_avg['wind_max_avg'].isnull(),['wind_max_avg','wind_max_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_wind_max_avg= r2_score(pre_wind_max_avg['wind_max_avg'], pre_wind_max_avg['wind_max_avg_imputed'])\n",
    "msq_wind_max_avg= mean_squared_error(pre_wind_max_avg['wind_max_avg'], pre_wind_max_avg['wind_max_avg_imputed'])\n",
    "\n",
    "seasons_17= imputer_wind_max_avg.predict(seasons_16.loc[seasons_16['wind_max_avg'].isnull(),:])\n",
    "del seasons_17[\"wind_max_avg\"]\n",
    "seasons_17=seasons_17.rename(columns={'wind_max_avg_imputed':'wind_max_avg'}).append(seasons_16.loc[~seasons['wind_max_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "#sun----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_17, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_sun= SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_avg'],\n",
    "output_column='sun',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_sun.fit(train_df=df_train)\n",
    "predictions_sun= imputer_sun.predict(df_test)\n",
    "\n",
    "pre_sun=predictions_sun.loc[~predictions_sun['sun'].isnull(),['sun','sun_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_sun= r2_score(pre_sun['sun'], pre_sun['sun_imputed'])\n",
    "msq_sun= mean_squared_error(pre_sun['sun'], pre_sun['sun_imputed'])\n",
    "\n",
    "seasons_18= imputer_sun.predict(seasons_17.loc[seasons_17['sun'].isnull(),:])\n",
    "del seasons_18[\"sun\"]\n",
    "seasons_18=seasons_18.rename(columns={'sun_imputed':'sun'}).append(seasons_17.loc[~seasons['sun'].isnull(),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:43:23.819381Z",
     "start_time": "2021-06-06T20:43:23.570744Z"
    }
   },
   "outputs": [],
   "source": [
    "seasons_18.to_csv('seasons_impute.csv')\n",
    "seasons_18.to_csv('../data/seasons_impute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:43:23.834525Z",
     "start_time": "2021-06-06T20:43:23.822164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' estacion', 'code_merge', 'codigo', 'freez', 'freez_imputed_proba',\n",
       "       'hum', 'hum_imputed_proba', 'lev_max', 'lev_max_imputed_proba',\n",
       "       'lev_mid', 'lev_mid_imputed_proba', 'lev_min', 'lev_min_imputed_proba',\n",
       "       'merge_cod', 'month', 'rain', 'rain_1mm', 'rain_1mm_imputed_proba',\n",
       "       'rain_cum', 'rain_cum_imputed_proba', 'rain_imputed_proba',\n",
       "       'rain_max_10', 'rain_max_10_imputed_proba', 'rain_max_day',\n",
       "       'rain_max_day_imputed_proba', 'sun', 'sun_imputed_proba', 'temp_avg',\n",
       "       'temp_avg_imputed_proba', 'temp_max_abs', 'temp_max_abs_imputed_proba',\n",
       "       'temp_max_avg', 'temp_max_avg_imputed_proba', 'temp_min_abs',\n",
       "       'temp_min_abs_imputed_proba', 'wind_avg', 'wind_avg_imputed_proba',\n",
       "       'wind_max', 'wind_max_avg', 'wind_max_avg_imputed_proba',\n",
       "       'wind_max_imputed_proba', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imp_season= pd.read_csv('seasons_impute.csv')\n",
    "imp_season = seasons_18.copy()\n",
    "imp_season.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generate the YEARLY dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:43:23.922515Z",
     "start_time": "2021-06-06T20:43:23.836297Z"
    }
   },
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-acd4acc1ed01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                          \u001b[0;34m'wind_avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                          \u001b[0;34m'wind_max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                          'wind_max_avg':'max'}).reset_index()\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aggregate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_agg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_agg_1dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_agg\u001b[0;34m(arg, func)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_how\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_how\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_agg_1dim\u001b[0;34m(name, how, subset)\u001b[0m\n\u001b[1;32m    438\u001b[0m                         \u001b[0;34m\"nested dictionary is ambiguous \"\u001b[0m \u001b[0;34m\"in aggregation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     )\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcolg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_level\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_agg_2dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func_or_funcs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_or_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             return self._cython_agg_general(\n\u001b[0;32m-> 1205\u001b[0;31m                 \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m             )\n\u001b[1;32m   1207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No numeric types to aggregate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_aggregated_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "#2017\n",
    "imp_season_anual_17= imp_season.loc[imp_season.year==2017,['codigo', 'freez', 'hum','lev_max', 'lev_mid', 'lev_min', 'rain',\n",
    "                                'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
    "                                'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg','wind_max', 'wind_max_avg','year']]\n",
    "\n",
    "imp_season_anual_17=imp_season_anual_17.groupby(['codigo','year'],as_index=True ).agg({'freez':'mean', \n",
    "                         'hum':'mean', \n",
    "                         'lev_max':'max', \n",
    "                         'lev_mid':'mean',\n",
    "                         'lev_min':'min', \n",
    "                         'rain':'mean', \n",
    "                         'rain_1mm':'mean',\n",
    "                         'rain_cum':'mean', \n",
    "                         'rain_max_10':'max', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'sun':'mean',\n",
    "                         'temp_avg':'mean',\n",
    "                         'temp_max_abs':'max',\n",
    "                         'temp_max_avg':'max', \n",
    "                         'temp_min_abs':'min', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'wind_avg':'mean',\n",
    "                         'wind_max':'max',                                            \n",
    "                         'wind_max_avg':'max'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "#2018\n",
    "imp_season_anual_18= imp_season.loc[imp_season.year==2018,['codigo', 'freez', 'hum','lev_max', 'lev_mid', 'lev_min', 'rain',\n",
    "                                'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
    "                                'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg','wind_max', 'wind_max_avg','year']]\n",
    "\n",
    "imp_season_anual_18=imp_season_anual_18.groupby(['codigo','year'],as_index=True ).agg({'freez':'mean', \n",
    "                         'hum':'mean', \n",
    "                         'lev_max':'max', \n",
    "                         'lev_mid':'mean',\n",
    "                         'lev_min':'min', \n",
    "                         'rain':'mean', \n",
    "                         'rain_1mm':'mean',\n",
    "                         'rain_cum':'mean', \n",
    "                         'rain_max_10':'max', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'sun':'mean',\n",
    "                         'temp_avg':'mean',\n",
    "                         'temp_max_abs':'max',\n",
    "                         'temp_max_avg':'max', \n",
    "                         'temp_min_abs':'min', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'wind_avg':'mean',\n",
    "                         'wind_max':'max',                                            \n",
    "                         'wind_max_avg':'max'}).reset_index()\n",
    "\n",
    "#2019\n",
    "imp_season_anual_19= imp_season.loc[imp_season.year==2019,['codigo', 'freez', 'hum','lev_max', 'lev_mid', 'lev_min', 'rain',\n",
    "                                'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
    "                                'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg','wind_max', 'wind_max_avg','year']]\n",
    "\n",
    "imp_season_anual_19=imp_season_anual_19.groupby(['codigo','year'],as_index=True ).agg({'freez':'mean', \n",
    "                         'hum':'mean', \n",
    "                         'lev_max':'max', \n",
    "                         'lev_mid':'mean',\n",
    "                         'lev_min':'min', \n",
    "                         'rain':'mean', \n",
    "                         'rain_1mm':'mean',\n",
    "                         'rain_cum':'mean', \n",
    "                         'rain_max_10':'max', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'sun':'mean',\n",
    "                         'temp_avg':'mean',\n",
    "                         'temp_max_abs':'max',\n",
    "                         'temp_max_avg':'max', \n",
    "                         'temp_min_abs':'min', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'wind_avg':'mean',\n",
    "                         'wind_max':'max',                                            \n",
    "                         'wind_max_avg':'max'}).reset_index()\n",
    "\n",
    "imp_season_anual= imp_season_anual_17.append(imp_season_anual_18).append(imp_season_anual_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T20:43:23.927571Z",
     "start_time": "2021-06-06T20:20:14.106Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_season_anual.to_csv('WBds02_METEO.csv',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
